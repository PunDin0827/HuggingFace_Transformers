{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZlg4YVDxMaA8J00cUtAbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PunDin0827/HuggingFace_Transformers/blob/main/token_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.1 導入相關套件*"
      ],
      "metadata": {
        "id": "NsdwjJwpdLEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "38eWaX3RcqQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "BZRDTGTRciSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTMaiBZsbn5v"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer,AutoModelForTokenClassification,TrainingArguments,Trainer,DataCollatorForTokenClassification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.2 載入數據*"
      ],
      "metadata": {
        "id": "bnDHi7RMdRk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_datasets = load_dataset(\"peoples_daily_ner\" ,cache_dir=\"./data\")\n",
        "ner_datasets"
      ],
      "metadata": {
        "id": "H_0MuPo4cflG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "w-VWgLVad5vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_datasets[\"train\"].features"
      ],
      "metadata": {
        "id": "joOWEcjpePvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = ner_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_list"
      ],
      "metadata": {
        "id": "ZqQkf5MHe0ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.3 數據處理*"
      ],
      "metadata": {
        "id": "yuh-wi5wfktF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
      ],
      "metadata": {
        "id": "E45dCtGifoNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(ner_datasets[\"train\"][0][\"tokens\"],is_split_into_words=True)"
      ],
      "metadata": {
        "id": "K7WPnpfwgm88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_ids label\n",
        "def process_function(examples):\n",
        "  tokenized_expamles = tokenizer(examples[\"tokens\"], max_length=128 , truncation=True , is_split_into_words=True)\n",
        "  labels=[]\n",
        "  for i ,label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_expamles.word_ids(batch_index=i)\n",
        "    label_ids=[]\n",
        "    for word_id in word_ids:\n",
        "      if word_id is None:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_id])\n",
        "    labels.append(label_ids)\n",
        "  tokenized_expamles[\"lables\"] = labels\n",
        "  return tokenized_expamles"
      ],
      "metadata": {
        "id": "ZLiNdl4fhABr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_datasets = ner_datasets.map(process_function , batched=True)\n",
        "tokenizer_datasets"
      ],
      "metadata": {
        "id": "zvJr9IvHi6wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_datasets[\"train\"][0])"
      ],
      "metadata": {
        "id": "xSNlr8JsjFuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.4 建立模型*"
      ],
      "metadata": {
        "id": "ug8e3pAvoMhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 將二分類轉換成多分類\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"hfl/chinese-macbert-base\",num_labels=len(label_list))"
      ],
      "metadata": {
        "id": "2TQ0WnN1oQTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.num_labels"
      ],
      "metadata": {
        "id": "GYnXVu71yCj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step.5 評估函數*"
      ],
      "metadata": {
        "id": "QljwGK7qojF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqeval = evaluate.load(\"seqeval\")\n",
        "seqeval"
      ],
      "metadata": {
        "id": "Qp40Zj1nolG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def eval_metric(pred):\n",
        "  predictions , labels = pred\n",
        "  predictions.np.argmax(predictions, axis=-1)\n",
        "\n",
        "  # 將id轉換成原始字串類型\n",
        "  true_predictions = [\n",
        "      [lable_list[p] for p , l in zip(prediction,label) if l != -100]\n",
        "      for prediction , label in zip(predictions,labels)\n",
        "  ]\n",
        "\n",
        "  true_labels = [\n",
        "      [label_list[l] for p , l in zip(prediction,label) if l != -100]\n",
        "      for prediction , label in zip(predictions,labels)\n",
        "  ]\n",
        "\n",
        "  result = seqeval.compute(predictions=true_predictions , references=true_labels , model=\"strict\" , scheme=\"IOB2\")\n",
        "\n",
        "  return{\n",
        "      \"f1\":result[\"overall_f1\"]\n",
        "  }"
      ],
      "metadata": {
        "id": "PCP2qJ5QrA8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step.6 訓練參數*"
      ],
      "metadata": {
        "id": "b0aN4TIMtd8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir =\"models_for_ner\",\n",
        "    per_device_train_batch_size = 64,\n",
        "    per_device_eval_batch_size = 128,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    metric_for_best_model = \"f1\",\n",
        "    load_best_model_at_end = True,\n",
        "    logging_steps = 50,\n",
        "    num_train_epoch = 3\n",
        ")"
      ],
      "metadata": {
        "id": "DNpjFyX8tqs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step.7 訓練器"
      ],
      "metadata": {
        "id": "u3NfEDQevDlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = args,\n",
        "    train_dataset = tokenizer_datasets[\"train\"],\n",
        "    eval_dataset = tokenizer_datasets[\"validation\"],\n",
        "    compute_metrics =eval_metric,\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "AdrxjO-UvNc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.8 模型訓練*"
      ],
      "metadata": {
        "id": "uWI3GG0uxgML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2ratF0F2xl6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])"
      ],
      "metadata": {
        "id": "Hg9D-VEK6Q5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.9 模型預測*"
      ],
      "metadata": {
        "id": "DeQ3is4v6LUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "Hsd7IWM46FZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用pipline推理要指定ld2label\n",
        "model.config.id2label = {idx: label for idx, label in enumerate(label_list)}\n",
        "model.config"
      ],
      "metadata": {
        "id": "cbTpFW1F6T9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定aggregation_strategy=simple得到實體結果，而非token\n",
        "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "id": "VajhZAa56V-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = ner_pipe(\"小明在北京上班\")\n",
        "res"
      ],
      "metadata": {
        "id": "fgmHGSxy6XTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 根據start和end取得實際結果\n",
        "ner_result = {}\n",
        "x = \"小明在北京上班\"\n",
        "for r in res:\n",
        "    if r[\"entity_group\"] not in ner_result:\n",
        "        ner_result[r[\"entity_group\"]] = []\n",
        "    ner_result[r[\"entity_group\"]].append(x[r[\"start\"]: r[\"end\"]])\n",
        "\n",
        "ner_result"
      ],
      "metadata": {
        "id": "cHgvKey96Yvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
